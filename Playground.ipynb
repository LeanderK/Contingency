{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# %load contingency.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random as nprandom\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "num_adversarial = 10 #how many adversarial examples should be generated (if any) per iteration\n",
    "\n",
    "# Adapted from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "#TODO max size?\n",
    "contingency_imges = np.empty(shape=(0, num_input))\n",
    "contingency_labels = np.empty(shape=(0))\n",
    "\n",
    "def reset_contingency():\n",
    "    global contingency_imges\n",
    "    contingency_imges = np.empty(shape=(0, num_input))\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, is_training, should_reuse):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=should_reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Define the model function\n",
    "def model_fn(features, labels, is_training, should_reuse):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits = conv_net(features, num_classes, dropout, is_training=is_training, should_reuse=should_reuse)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    correct_prediction = tf.equal(pred_classes, tf.cast(labels, dtype=tf.int64))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    #acc, acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    return (loss_op, pred_classes, accuracy)\n",
    "\n",
    "def withoutContingency(features, labels, is_training):\n",
    "    (loss_op, pred, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainWithout(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                is_training.name: True})  \n",
    "        empty_imges = np.empty(shape=(0, num_input))\n",
    "        empty_labels = np.empty(shape=(0))\n",
    "        empty = (empty_imges, empty_labels)\n",
    "        return (a, empty)\n",
    "       \n",
    "    return (acc, pred, trainWithout)\n",
    "\n",
    "def withRandomContingency(features, labels, is_training):\n",
    "    (orig_loss_op, pred, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    cont_batch = tf.placeholder(dtype=tf.float32, shape=[None, num_input], name=\"cont_batch\")\n",
    "    cont_batch_la = tf.placeholder(dtype=tf.float32, shape=[None], name=\"cont_batch_labels\")\n",
    "\n",
    "    #scalar that controls contingency \n",
    "    cont_beta = tf.placeholder(dtype=tf.float32, shape=[], name=\"cont_beta\")\n",
    "\n",
    "    (cont_loss, cont_pred, cont_acc) = model_fn(cont_batch, cont_batch_la, is_training, True)\n",
    "    loss_with_cont = orig_loss_op + cont_beta * cont_loss\n",
    "    train_op = optimizer.minimize(loss_with_cont,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainingRandomStep(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        randomImages = nprandom.random((num_adversarial, num_input))\n",
    "        randlabels = np.zeros(num_adversarial)\n",
    "        (cont_train_imges, cont_train_labels) = cont_training\n",
    "        cont_imges = np.concatenate((cont_train_imges,randomImages), axis=0)\n",
    "        cont_labels = np.concatenate((cont_train_labels,randlabels), axis=0)\n",
    "\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                cont_batch.name: cont_imges,\n",
    "                cont_batch_la.name: cont_labels,\n",
    "                cont_beta.name: 1,\n",
    "                is_training.name: True})  \n",
    "        random_cont = (randomImages, randlabels)\n",
    "        return (a, random_cont)\n",
    "    return (acc, pred, trainingRandomStep)\n",
    "\n",
    "def withContingency(features, labels, is_training):\n",
    "    (orig_loss_op, pred, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    cont_batch = tf.placeholder(dtype=tf.float32, shape=[None, num_input], name=\"cont_batch\")\n",
    "    cont_batch_la = tf.placeholder(dtype=tf.float32, shape=[None], name=\"cont_batch_labels\")\n",
    "\n",
    "    #scalar that controls contingency \n",
    "    cont_beta = tf.placeholder(dtype=tf.float32, shape=[], name=\"cont_beta\")\n",
    "\n",
    "    (cont_loss, cont_pred, cont_acc) = model_fn(cont_batch, cont_batch_la, is_training, True)\n",
    "    loss_with_cont = orig_loss_op + cont_beta * cont_loss\n",
    "    train_op = optimizer.minimize(loss_with_cont,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    #generating the contingengy\n",
    "    gen_images = tf.get_variable(dtype=tf.float32, shape=[num_adversarial, num_input], name=\"gen_images\")\n",
    "    gen_labels = tf.placeholder(dtype=tf.float32, shape=[num_adversarial], name=\"gen_labels\")\n",
    "\n",
    "    (gen_loss_op, gen_pred, gen_acc) = model_fn(gen_images, gen_labels, is_training, True)\n",
    "\n",
    "    #TODO replace\n",
    "    #there is a general mysterium surrounding this function. What does it do exactly? I have not get round \n",
    "    #testing/investigating it yet.\n",
    "    diff = tf.contrib.gan.eval.frechet_classifier_distance(\n",
    "        features, \n",
    "        gen_images, \n",
    "        #I don't really understand what this is used for...\n",
    "        lambda imges : conv_net(imges, num_classes, False, is_training=is_training, should_reuse=True))\n",
    "\n",
    "    adversial_fitness = gen_loss_op #- diff\n",
    "    #TODO maybe switch to Adam and reset it for each (classifier-)training step? \n",
    "    optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    adv_op = optimizer2.minimize(-1 * adversial_fitness,\n",
    "                                global_step=tf.train.get_global_step(),\n",
    "                                var_list=gen_images)\n",
    "    numGradAscentIter = 5\n",
    "    def trainingContStep(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        (cont_img, cont_labels) = cont_training\n",
    "\n",
    "        # generates the congingency\n",
    "        randomImages = nprandom.random((num_adversarial, num_input))\n",
    "        zerolabels = np.zeros(num_adversarial)\n",
    "        session.run(gen_images.assign(randomImages))\n",
    "\n",
    "        for iteration in range(numGradAscentIter):\n",
    "            #TODO does this work?\n",
    "            a = session.run(adv_op, feed_dict={\n",
    "                    gen_labels.name: zerolabels,\n",
    "                    #we are not training the weights!\n",
    "                    is_training.name: False})\n",
    "\n",
    "        adv_images = session.run(gen_images)\n",
    "        cont_img = np.concatenate((cont_img, adv_images), axis=0)\n",
    "        #TODO redo this, we can't switch contingency labels right now\n",
    "        cont_labels = np.zeros(cont_img.shape[0])\n",
    "\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                cont_batch.name: cont_img,\n",
    "                cont_batch_la.name: cont_labels,\n",
    "                cont_beta.name: 1,\n",
    "                is_training.name: True})  \n",
    "\n",
    "        return (a, (adv_images, zerolabels))\n",
    "    return (acc, pred, trainingContStep)\n",
    "\n",
    "def run(model_fn): \n",
    "    images = tf.placeholder(tf.float32, shape=[None, num_input], name=\"images\")\n",
    "    labels = tf.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    with tf.Session() as session:\n",
    "        (acc_eval, pred_eval, train_fn) = model_fn(images, labels, is_training)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        # Build the Estimator\n",
    "\n",
    "        for iteration in range(num_steps):\n",
    "            (training, cont_training) = next_batch(batch_size, (batch_size - num_adversarial))\n",
    "            (a, cont) = train_fn(iteration, session, training, cont_training)\n",
    "            (cont_imgs, cont_la) = cont\n",
    "            global contingency_imges\n",
    "            global contingency_labels\n",
    "            contingency_imges = np.concatenate((contingency_imges, cont_imgs), axis=0)\n",
    "            contingency_labels = np.concatenate((contingency_labels, cont_la), axis=0)\n",
    "            # a = session.run(accEval, feed_dict={images.name: train_im, labels.name: train_la})\n",
    "            if iteration % 50 == 0:\n",
    "                print(\"Generated contingency in iteration \", iteration, \":\", contingency_imges.shape[0])\n",
    "                print(\"Training Accuracy in iteration \", iteration, \":\", a)\n",
    "\n",
    "        (eval_rel_im, eval_rel_la) = relabel(mnist.test)\n",
    "        (eval_valid_im, eval_valid_la) = only_valid(mnist.test.images, mnist.test.labels)\n",
    "        #a = session.run(acc_eval, feed_dict={images: eval_rel_im, labels: eval_rel_la, is_training.name: False})\n",
    "        #print(\"Final Accuracy on all relabeled classes\", iteration, \":\", a)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_valid_im, labels: eval_valid_la, is_training.name: False})\n",
    "        print(\"Final Accuracy on only valid classes\", iteration, \":\", a)\n",
    "\n",
    "        #TODO ugly code...\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        # BEWARE: the zero class here is unexpected input, not the zero-mnist class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "\n",
    "        \n",
    "        # from: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        # BEWARE: the zero class here is unexpected input, not the zero-mnist class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        (data, labels) = relabel_roc(mnist.test)\n",
    "        pred = session.run(pred_eval, \n",
    "                feed_dict={images: data, is_training.name: False})\n",
    "        pred_roc = relabel_pred_roc(pred)\n",
    "        print(\"#1\", pred.shape, \"#2\", pred_roc.shape, \"#3\", mnist.test.labels.shape, \"#4\", labels.shape)\n",
    "        fpr[0], tpr[0], _ = roc_curve(labels, pred_roc)\n",
    "        roc_auc[0] = auc(fpr[0], tpr[0])\n",
    "\n",
    "        return (fpr, tpr, roc_auc)\n",
    "\n",
    "def only_valid(images, labels):\n",
    "    indices = np.where(labels < num_classes )\n",
    "    return (images[indices], labels[indices])\n",
    "\n",
    "def relabel(dataset):\n",
    "    indices = np.where(dataset.labels >= num_classes )\n",
    "    relabel = np.copy(dataset.labels)\n",
    "    relabel[indices] = 0\n",
    "    return (dataset.images, relabel)\n",
    "\n",
    "def relabel_roc(dataset):\n",
    "    not_null = np.where(dataset.labels > 0)\n",
    "    imges = dataset.images[not_null]\n",
    "    labels = dataset.labels[not_null]\n",
    "    indices = np.where(labels >= num_classes )\n",
    "    labels[indices] = 0\n",
    "    mask = np.ones(labels.shape,dtype=bool)\n",
    "    mask[indices] = False\n",
    "    labels[mask] = 1\n",
    "    return (dataset.images, labels)\n",
    "\n",
    "def relabel_pred_roc(pred):\n",
    "    #work around 'builtin_function_or_method' object does not support item assignment\n",
    "    indices = np.where(pred > 0)\n",
    "    rel_pred = np.zeros(pred.shape,dtype=int)\n",
    "    rel_pred[indices] = 1\n",
    "    return rel_pred\n",
    "\n",
    "def next_batch(batch_size_training, batch_size_contingency):\n",
    "    training = only_valid(*mnist.train.next_batch(batch_size))\n",
    "    indices = np.arange(0 , contingency_imges.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:batch_size_contingency]\n",
    "    contingency_trainig_imgs = contingency_imges[indices]\n",
    "    contingency_trainig_labels = contingency_labels[indices]\n",
    "    contingency = (contingency_trainig_imgs, contingency_trainig_labels)\n",
    "    return (training, contingency)\n",
    "\n",
    "def test_data_for_label(label):\n",
    "    indices = np.where(mnist.test.labels == label)\n",
    "    return (mnist.test.images[indices], mnist.test.labels[indices])\n",
    "\n",
    "def unexpected_data():\n",
    "    indices = np.where(mnist.test.labels >= num_classes)\n",
    "    imges = mnist.test.images[indices]\n",
    "    #TODO more dynamic defaults\n",
    "    zeros = np.zeros(indices[0].shape[0])\n",
    "    return (imges, zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 10\n",
      "Training Accuracy in iteration  0 : 0.484848\n",
      "Generated contingency in iteration  50 : 510\n",
      "Training Accuracy in iteration  50 : 0.961538\n",
      "Generated contingency in iteration  100 : 1010\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 1510\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on only valid classes 199 : 0.998582\n",
      "#1 (10000,) #2 (10000,) #3 (10000,) #4 (9020,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9020, 10000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-c8ec223459e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreset_contingency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithContingency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-05e7d121c702>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model_fn)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mpred_roc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelabel_pred_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_roc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_roc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9020, 10000]"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reset_contingency()\n",
    "(fpr, tpr, roc_auc) = run(withContingency)\n",
    "plot_roc(i, fpr, tpr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 0\n",
      "Training Accuracy in iteration  0 : 0.590909\n",
      "Generated contingency in iteration  50 : 0\n",
      "Training Accuracy in iteration  50 : 0.956522\n",
      "Generated contingency in iteration  100 : 0\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 0\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on all relabeled classes 199 : 0.7874\n",
      "Final Accuracy on only valid classes 199 : 0.999527\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reset_contingency()\n",
    "run(withoutContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 10\n",
      "Training Accuracy in iteration  0 : 0.633333\n",
      "Generated contingency in iteration  50 : 510\n",
      "Training Accuracy in iteration  50 : 0.961538\n",
      "Generated contingency in iteration  100 : 1010\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 1510\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on all relabeled classes 199 : 0.8876\n",
      "Final Accuracy on only valid classes 199 : 0.995272\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reset_contingency()\n",
    "run(withRandomContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LeanderK/Documents/ML/contingency/venv/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(i, fpr, tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[i], tpr[i], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7885,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(mnist.test.labels >= num_classes)[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2,    4,   11, ..., 9000, 9005, 9014]),)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(imges, lables) = relabel_roc(mnist.test)\n",
    "np.where(lables == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
