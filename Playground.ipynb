{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# %load contingency.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# Adapted from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random as nprandom\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 5 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet'):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Define the model function\n",
    "def model_fn(features, labels, is_training):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits = conv_net(features, num_classes, dropout, is_training=is_training)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    correct_prediction = tf.equal(pred_classes, tf.cast(labels, dtype=tf.int64))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    #acc, acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    return (loss_op, accuracy)\n",
    "\n",
    "def withoutContingency(features, labels, is_training):\n",
    "    (loss_op, acc) = model_fn(features, labels, is_training)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainWithout(iteration, session, train_images, train_labels):\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                is_training.name: True})  \n",
    "        return a\n",
    "       \n",
    "    return (acc, trainWithout)\n",
    "\n",
    "def withRandomContingency(features, labels, is_training):\n",
    "    (loss_op, acc) = model_fn(features, labels, is_training)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainingRandomStep(iteration, session, train_images, train_labels):\n",
    "        randomImages = nprandom.random((30, num_input))\n",
    "        randlabels = np.zeros(30)\n",
    "        resultingImg = np.concatenate((train_images,randomImages))\n",
    "        resultingLab = np.concatenate((train_labels,randlabels))\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"resultingImgshape\", resultingImg.shape)\n",
    "            print(\"train_imagesshape\", train_images.shape)\n",
    "            print(\"resultingLabshape\", resultingLab.shape)\n",
    "            print(\"train_labelsshape\", train_labels.shape)\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: resultingImg,\n",
    "                labels.name: resultingLab,\n",
    "                is_training.name: True})  \n",
    "        return a\n",
    "    return (acc, trainingRandomStep)\n",
    "\n",
    "def withContingency(features, labels, is_training):\n",
    "    (loss_op, acc) = model_fn(features, labels, is_training)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "    gen_images = tf.placeholder(tf.float32, shape=[None, num_input], name=\"images\")\n",
    "    diff = tf.reduce_mean(tf.abs(features - gen_images))\n",
    "    print(\"resultingImgshape\", diff.shape)\n",
    "    max_fitness = loss_op - tf.reduce_mean(diff)\n",
    "    optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    gen_op = optimizer2.maximize(max_fitness,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "    def trainingRandomStep(iteration, session, train_images, train_labels):\n",
    "        randomImages = nprandom.random((30, num_input))\n",
    "        randlabels = np.zeros(30)\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            print(\"resultingImgshape\", resultingImg.shape)\n",
    "            print(\"train_imagesshape\", train_images.shape)\n",
    "            print(\"resultingLabshape\", resultingLab.shape)\n",
    "            print(\"train_labelsshape\", train_labels.shape)\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: resultingImg,\n",
    "                labels.name: resultingLab,\n",
    "                is_training.name: True})  \n",
    "        return a\n",
    "    return (acc, trainingRandomStep)\n",
    "\n",
    "def run(model_fn): \n",
    "    images = tf.placeholder(tf.float32, shape=[None, num_input], name=\"images\")\n",
    "    labels = tf.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    with tf.Session() as session:\n",
    "        (acc_eval, train_fn) = model_fn(images, labels, is_training)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        # Build the Estimator\n",
    "\n",
    "        for iteration in range(num_steps):\n",
    "            (train_im, train_la) = only_valid(*mnist.train.next_batch(batch_size))\n",
    "            a = train_fn(iteration, session, train_im, train_la)\n",
    "            # a = session.run(accEval, feed_dict={images.name: train_im, labels.name: train_la})\n",
    "            if iteration % 50 == 0:\n",
    "                print(\"Batch labels\", np.unique(train_la, return_counts=True))\n",
    "                print(\"Training Accuracy in iteration \", iteration, \":\", a)\n",
    "\n",
    "        (eval_im, eval_la) = relabel(mnist.test)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_im, labels: eval_la, is_training.name: False})\n",
    "        print(\"Final Accuracy \", iteration, \":\", a)\n",
    "\n",
    "def only_valid(images, labels):\n",
    "    indices = np.where(labels < num_classes )\n",
    "    return (images[indices], labels[indices])\n",
    "\n",
    "def relabel(dataset):\n",
    "    indices = np.where(dataset.labels >= num_classes )\n",
    "    relabel = np.copy(dataset.labels)\n",
    "    relabel[indices] = 0\n",
    "    return (dataset.images, relabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels (array([0, 1, 2, 3, 4], dtype=uint8), array([ 9, 19, 12, 13, 11]))\n",
      "Training Accuracy in iteration  0 : 0.21875\n",
      "Batch labels (array([0, 1, 2, 3, 4], dtype=uint8), array([16, 12, 12, 13, 16]))\n",
      "Training Accuracy in iteration  100 : 0.971014\n",
      "Final Accuracy  199 : 0.6239\n"
     ]
    }
   ],
   "source": [
    "run(withoutContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultingImgshape (101, 784)\n",
      "train_imagesshape (71, 784)\n",
      "resultingLabshape (101,)\n",
      "train_labelsshape (71,)\n",
      "Batch labels (array([0, 1, 2, 3, 4], dtype=uint8), array([15, 13, 12, 13, 18]))\n",
      "Training Accuracy in iteration  0 : 0.237624\n",
      "resultingImgshape (85, 784)\n",
      "train_imagesshape (55, 784)\n",
      "resultingLabshape (85,)\n",
      "train_labelsshape (55,)\n",
      "Batch labels (array([0, 1, 2, 3, 4], dtype=uint8), array([ 5, 12,  9, 12, 17]))\n",
      "Training Accuracy in iteration  50 : 1.0\n",
      "resultingImgshape (88, 784)\n",
      "train_imagesshape (58, 784)\n",
      "resultingLabshape (88,)\n",
      "train_labelsshape (58,)\n",
      "Batch labels (array([0, 1, 2, 3, 4], dtype=uint8), array([19,  9, 13, 11,  6]))\n",
      "Training Accuracy in iteration  100 : 0.988636\n"
     ]
    }
   ],
   "source": [
    "run(withRandomContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultingImgshape ()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GradientDescentOptimizer' object has no attribute 'maximize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6e34cf23d871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithContingency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b905caa33c78>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model_fn)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"is_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0macc_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b905caa33c78>\u001b[0m in \u001b[0;36mwithContingency\u001b[0;34m(features, labels, is_training)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mmax_fitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0moptimizer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     gen_op = optimizer2.maximize(max_fitness,\n\u001b[0m\u001b[1;32m    137\u001b[0m                                 global_step=tf.train.get_global_step())\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrainingRandomStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientDescentOptimizer' object has no attribute 'maximize'"
     ]
    }
   ],
   "source": [
    "run(withContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_im, train_la) = only_zero_one(*mnist.train.next_batch(batch_size))\n",
    "train_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
