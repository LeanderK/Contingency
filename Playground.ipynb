{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# %load contingency.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random as nprandom\n",
    "\n",
    "# Adapted from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "#TODO max size?\n",
    "contingency = np.empty(shape=(784, 0))\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 5 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, is_training, should_reuse):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=should_reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Define the model function\n",
    "def model_fn(features, labels, is_training, should_reuse):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits = conv_net(features, num_classes, dropout, is_training=is_training, should_reuse=should_reuse)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    correct_prediction = tf.equal(pred_classes, tf.cast(labels, dtype=tf.int64))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    #acc, acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    return (loss_op, accuracy)\n",
    "\n",
    "def withoutContingency(features, labels, is_training):\n",
    "    (loss_op, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainWithout(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                is_training.name: True})  \n",
    "        return (a, np.empty(shape=(784, 0)))\n",
    "       \n",
    "    return (acc, trainWithout)\n",
    "\n",
    "def withRandomContingency(features, labels, is_training):\n",
    "    (loss_op, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainingRandomStep(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        randomImages = nprandom.random((30, num_input))\n",
    "        randlabels = np.zeros(30)\n",
    "        resultingImg = np.concatenate((train_images,randomImages))\n",
    "        resultingLab = np.concatenate((train_labels,randlabels))\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: resultingImg,\n",
    "                labels.name: resultingLab,\n",
    "                is_training.name: True})  \n",
    "        return (a, np.empty(shape=(784, 0)))\n",
    "    return (acc, trainingRandomStep)\n",
    "\n",
    "def withContingency(features, labels, is_training):\n",
    "    (orig_loss_op, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    num_adversarial = 10\n",
    "    cont_batch = tf.get_variable(dtype=tf.float32, shape=[batch_size + num_adversarial, num_input], name=\"cont_batch\")\n",
    "    cont_batch_la = tf.get_variable(dtype=tf.float32, shape=[batch_size + num_adversarial], name=\"cont_batch_labels\")\n",
    "\n",
    "    #scalar that controls contingency \n",
    "    cont_beta = tf.get_variable(dtype=tf.float32, shape=[], name=\"cont_beta\")\n",
    "\n",
    "    loss_with_cont = orig_loss_op + cont_beta * model_fn(cont_batch, cont_batch_la, is_training, True)\n",
    "    train_op = optimizer.minimize(loss_with_cont,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    #generating the contingengy\n",
    "    gen_images = tf.get_variable(dtype=tf.float32, shape=[num_adversarial, num_input], name=\"gen_images\")\n",
    "    gen_labels = tf.get_variable(dtype=tf.float32, shape=[num_adversarial], name=\"gen_labels\")\n",
    "\n",
    "    (cont_loss_op, cont_acc) = model_fn(gen_images, gen_labels, is_training, True)\n",
    "\n",
    "    #there is a general mysterium surrounding this function. What does it do exactly? I have not get round \n",
    "    #testing/investigating it yet.\n",
    "    diff = tf.contrib.gan.eval.frechet_classifier_distance(\n",
    "        features, \n",
    "        gen_images, \n",
    "        #I don't really understand what this is used for...\n",
    "        lambda imges : conv_net(imges, num_classes, False, is_training=is_training, should_reuse=True))\n",
    "\n",
    "    adversial_fitness = cont_loss_op - diff\n",
    "    optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    adv_op = optimizer2.minimize(-1 * adversial_fitness,\n",
    "                                global_step=tf.train.get_global_step(),\n",
    "                                var_list=gen_images)\n",
    "    numGradAscentIter = 5\n",
    "    def trainingContStep(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        (cont_img, cont_labels) = cont_training\n",
    "\n",
    "        # generates the congingency\n",
    "        randomImages = nprandom.random((num_adversarial, num_input))\n",
    "        zerolabels = np.zeros(num_adversarial)\n",
    "        session.run(gen_images.assign(randomImages))\n",
    "\n",
    "        for iteration in range(numGradAscentIter):\n",
    "            #TODO does this work?\n",
    "            a = session.run(adv_op, feed_dict={\n",
    "                    gen_images.name: train_images,\n",
    "                    gen_labels.name: train_labels,\n",
    "                    is_training.name: False})\n",
    "\n",
    "        adv_images = session.run(gen_images)\n",
    "        cont_img = np.concatenate(cont_img, adv_images)\n",
    "        cont_labels = np.concatenate(cont_labels, zerolabels)\n",
    "\n",
    "        #fill with random to archieve a static graph (is only relevant for the initialization)\n",
    "        to_fill = max(0, (batch_size + num_adversarial) - len(cont_img))\n",
    "        to_fill_images = nprandom.random((to_fill, num_input))\n",
    "        cont_img = np.concatenate(cont_img, randomImages)\n",
    "        to_fill_zerolabels = np.zeros(to_fill)\n",
    "        cont_labels = np.concatenate(cont_labels, to_fill_zerolabels)\n",
    "\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                cont_batch.name: cont_img,\n",
    "                cont_batch_la.name: cont_labels,\n",
    "                cont_beta.name: 1,\n",
    "                is_training.name: True})  \n",
    "        return (a, adv_images)\n",
    "    return (acc, trainingRandomStep)\n",
    "\n",
    "def run(model_fn): \n",
    "    images = tf.placeholder(tf.float32, shape=[None, num_input], name=\"images\")\n",
    "    labels = tf.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    with tf.Session() as session:\n",
    "        (acc_eval, train_fn) = model_fn(images, labels, is_training)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        # Build the Estimator\n",
    "\n",
    "        for iteration in range(num_steps):\n",
    "            (training, cont_training) = next_batch(batch_size, batch_size)\n",
    "            (a, cont) = train_fn(iteration, session, training, cont_training)\n",
    "            global contingency\n",
    "            contingency = np.concatenate((contingency, cont))\n",
    "            # a = session.run(accEval, feed_dict={images.name: train_im, labels.name: train_la})\n",
    "            if iteration % 50 == 0:\n",
    "                print(\"Generated contingency in iteration \", iteration, \":\", len(contingency))\n",
    "                print(\"Training Accuracy in iteration \", iteration, \":\", a)\n",
    "\n",
    "        (eval_rel_im, eval_rel_la) = relabel(mnist.test)\n",
    "        (eval_valid_im, eval_valid_la) = only_valid(mnist.test.images, mnist.test.labels)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_rel_im, labels: eval_rel_la, is_training.name: False})\n",
    "        print(\"Final Accuracy on all relabeled classes\", iteration, \":\", a)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_valid_im, labels: eval_valid_la, is_training.name: False})\n",
    "        print(\"Final Accuracy on only valid classes\", iteration, \":\", a)\n",
    "        # TODO ROC-Courve\n",
    "\n",
    "def only_valid(images, labels):\n",
    "    indices = np.where(labels < num_classes )\n",
    "    return (images[indices], labels[indices])\n",
    "\n",
    "def relabel(dataset):\n",
    "    indices = np.where(dataset.labels >= num_classes )\n",
    "    relabel = np.copy(dataset.labels)\n",
    "    relabel[indices] = 0\n",
    "    return (dataset.images, relabel)\n",
    "\n",
    "def next_batch(batch_size_training, batch_size_contingency):\n",
    "    training = only_valid(*mnist.train.next_batch(batch_size))\n",
    "    indices = np.arange(0 , len(contingency))\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:batch_size_contingency]\n",
    "    contingency_trainig = contingency[indices]\n",
    "    return (training, contingency_trainig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation 'Svd_1' (op type: Svd)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m               \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_gradient_function\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2133\u001b[0m     \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     92\u001b[0m       raise LookupError(\n\u001b[0;32m---> 93\u001b[0;31m           \"%s registry has no entry for: %s\" % (self._name, name))\n\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m: gradient registry has no entry for: Svd",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8edd6bfbdf32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithContingency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-1f228d40ec1f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model_fn)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"is_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0macc_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1f228d40ec1f>\u001b[0m in \u001b[0;36mwithContingency\u001b[0;34m(features, labels, is_training)\u001b[0m\n\u001b[1;32m    154\u001b[0m     adv_op = optimizer2.minimize(-1 * adversial_fitness,\n\u001b[1;32m    155\u001b[0m                                 \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                                 var_list=gen_images)\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mnumGradAscentIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrainingContStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/contingency/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    553\u001b[0m               raise LookupError(\n\u001b[1;32m    554\u001b[0m                   \u001b[0;34m\"No gradient defined for operation '%s' (op type: %s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                   (op.name, op.type))\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m           \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnterGradWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: No gradient defined for operation 'Svd_1' (op type: Svd)"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(withContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 1568\n",
      "Training Accuracy in iteration  0 : 0.177419\n",
      "Generated contingency in iteration  50 : 40768\n",
      "Training Accuracy in iteration  50 : 0.984615\n",
      "Generated contingency in iteration  100 : 79968\n",
      "Training Accuracy in iteration  100 : 0.986111\n",
      "Generated contingency in iteration  150 : 119168\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on all relabeled classes 199 : 0.6136\n",
      "Final Accuracy on only valid classes 199 : 0.992411\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(withoutContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 158368\n",
      "Training Accuracy in iteration  0 : 0.226667\n",
      "Generated contingency in iteration  50 : 197568\n",
      "Training Accuracy in iteration  50 : 0.978495\n",
      "Generated contingency in iteration  100 : 236768\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 275968\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on all relabeled classes 199 : 0.5947\n",
      "Final Accuracy on only valid classes 199 : 0.994551\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(withRandomContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_im, train_la) = only_zero_one(*mnist.train.next_batch(batch_size))\n",
    "train_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
