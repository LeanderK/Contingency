{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# %load contingency.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# Adapted from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random as nprandom\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 300\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet'):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Define the model function\n",
    "def model_fn(features, labels, is_training):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits = conv_net(features, num_classes, dropout, is_training=is_training)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    acc, acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    return (loss_op, acc, acc_op)\n",
    "\n",
    "def withoutContingency(features, labels, is_training):\n",
    "    (loss_op, acc, acc_op) = model_fn(features, labels, is_training)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainWithout(iteration, session, train_images, train_labels):\n",
    "        a, a_op, t = session.run([acc, acc_op, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                is_training.name: True})  \n",
    "        return a\n",
    "       \n",
    "    return (acc, trainWithout)\n",
    "\n",
    "def withRandomContingency(features, labels):\n",
    "    (loss_op, acc, acc_op) = model_fn(features, labels, is_training)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainingRandomStep(iteration, session, train_images, train_labels):\n",
    "        randomImages = nprandom.random(num_input, 5)\n",
    "        labels = np.zeroes(5)\n",
    "        resultingImg = train_images.concatenate(randomImages)\n",
    "        resultingLab = train_labels.concatenate(labels)\n",
    "        a, a_op, t = session.run([acc, acc_op, train_op], feed_dict={\n",
    "                features.name: resultingImg,\n",
    "                labels.name: resultingLab,\n",
    "                is_training.name: True})  \n",
    "        return a\n",
    "    return (acc, trainingRandomStep)\n",
    "\n",
    "def run(model_fn): \n",
    "    images = tf.placeholder(tf.float32, shape=[None, num_input], name=\"images\")\n",
    "    labels = tf.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    with tf.Session() as session:\n",
    "        (acc_eval, train_fn) = model_fn(images, labels, is_training)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        # Build the Estimator\n",
    "\n",
    "        for iteration in range(num_steps):\n",
    "            (train_im, train_la) = only_zero_one(*mnist.train.next_batch(batch_size))\n",
    "            a = train_fn(iteration, session, train_im, train_la)\n",
    "            # a = session.run(accEval, feed_dict={images.name: train_im, labels.name: train_la})\n",
    "            if iteration % 100 == 0:\n",
    "                print(\"Training Accuracy in iteration \", iteration, \":\", a)\n",
    "\n",
    "        (eval_im, eval_la) = relabel(mnist.test)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_im, labels: eval_la, is_training.name: False})\n",
    "        print(\"Final Accuracy \", iteration, \":\", a)\n",
    "\n",
    "def only_zero_one(images, labels):\n",
    "    indices = np.where(labels <= 1 )\n",
    "    return (images[indices], labels[indices])\n",
    "\n",
    "def relabel(dataset):\n",
    "    indices = np.where(dataset.labels != 1 )\n",
    "    relabel = np.copy(dataset.labels)\n",
    "    relabel[indices] = 0\n",
    "    return (dataset.images, relabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy in iteration  0 : 0.0\n",
      "Training Accuracy in iteration  100 : 0.983007\n",
      "Training Accuracy in iteration  200 : 0.990939\n",
      "Final Accuracy  299 : 0.993056\n"
     ]
    }
   ],
   "source": [
    "run(withoutContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_im, train_la) = only_zero_one(*mnist.train.next_batch(batch_size))\n",
    "train_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
