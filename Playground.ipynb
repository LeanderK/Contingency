{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# %load contingency.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random as nprandom\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "num_adversarial = 10 #how many adversarial examples should be generated (if any) per iteration\n",
    "\n",
    "# Adapted from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "#TODO max size?\n",
    "contingency_imges = np.empty(shape=(0, num_input))\n",
    "contingency_labels = np.empty(shape=(0))\n",
    "\n",
    "def reset_contingency():\n",
    "    global contingency_imges\n",
    "    contingency_imges = np.empty(shape=(0, num_input))\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, is_training, should_reuse):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=should_reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Define the model function\n",
    "def model_fn(features, labels, is_training, should_reuse):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits = conv_net(features, num_classes, dropout, is_training=is_training, should_reuse=should_reuse)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    correct_prediction = tf.equal(pred_classes, tf.cast(labels, dtype=tf.int64))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    #acc, acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    return (loss_op, accuracy)\n",
    "\n",
    "def withoutContingency(features, labels, is_training):\n",
    "    (loss_op, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainWithout(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                is_training.name: True})  \n",
    "        empty_imges = np.empty(shape=(0, num_input))\n",
    "        empty_labels = np.empty(shape=(0))\n",
    "        empty = (empty_imges, empty_labels)\n",
    "        return (a, empty)\n",
    "       \n",
    "    return (acc, trainWithout)\n",
    "\n",
    "def withRandomContingency(features, labels, is_training):\n",
    "    (orig_loss_op, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    cont_batch = tf.placeholder(dtype=tf.float32, shape=[None, num_input], name=\"cont_batch\")\n",
    "    cont_batch_la = tf.placeholder(dtype=tf.float32, shape=[None], name=\"cont_batch_labels\")\n",
    "\n",
    "    #scalar that controls contingency \n",
    "    cont_beta = tf.placeholder(dtype=tf.float32, shape=[], name=\"cont_beta\")\n",
    "\n",
    "    loss_with_cont = orig_loss_op + cont_beta * model_fn(cont_batch, cont_batch_la, is_training, True)\n",
    "    train_op = optimizer.minimize(loss_with_cont,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    def trainingRandomStep(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        randomImages = nprandom.random((num_adversarial, num_input))\n",
    "        randlabels = np.zeros(num_adversarial)\n",
    "        (cont_train_imges, cont_train_labels) = cont_training\n",
    "        cont_imges = np.concatenate((cont_train_imges,randomImages), axis=0)\n",
    "        cont_labels = np.concatenate((cont_train_labels,randlabels), axis=0)\n",
    "\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                cont_batch.name: cont_imges,\n",
    "                cont_batch_la.name: cont_labels,\n",
    "                cont_beta.name: 1,\n",
    "                is_training.name: True})  \n",
    "        random_cont = (randomImages, randlabels)\n",
    "        return (a, random_cont)\n",
    "    return (acc, trainingRandomStep)\n",
    "\n",
    "def withContingency(features, labels, is_training):\n",
    "    (orig_loss_op, acc) = model_fn(features, labels, is_training, False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    cont_batch = tf.placeholder(dtype=tf.float32, shape=[None, num_input], name=\"cont_batch\")\n",
    "    cont_batch_la = tf.placeholder(dtype=tf.float32, shape=[None], name=\"cont_batch_labels\")\n",
    "\n",
    "    #scalar that controls contingency \n",
    "    cont_beta = tf.placeholder(dtype=tf.float32, shape=[], name=\"cont_beta\")\n",
    "\n",
    "    loss_with_cont = orig_loss_op + cont_beta * model_fn(cont_batch, cont_batch_la, is_training, True)\n",
    "    train_op = optimizer.minimize(loss_with_cont,\n",
    "                                global_step=tf.train.get_global_step())\n",
    "\n",
    "    #generating the contingengy\n",
    "    gen_images = tf.get_variable(dtype=tf.float32, shape=[num_adversarial, num_input], name=\"gen_images\")\n",
    "    gen_labels = tf.placeholder(dtype=tf.float32, shape=[num_adversarial], name=\"gen_labels\")\n",
    "\n",
    "    (cont_loss_op, cont_acc) = model_fn(gen_images, gen_labels, is_training, True)\n",
    "\n",
    "    #there is a general mysterium surrounding this function. What does it do exactly? I have not get round \n",
    "    #testing/investigating it yet.\n",
    "    diff = tf.contrib.gan.eval.frechet_classifier_distance(\n",
    "        features, \n",
    "        gen_images, \n",
    "        #I don't really understand what this is used for...\n",
    "        lambda imges : conv_net(imges, num_classes, False, is_training=is_training, should_reuse=True))\n",
    "\n",
    "    adversial_fitness = cont_loss_op #- diff\n",
    "    #TODO maybe switch to Adam and reset it for each (classifier-)training step? \n",
    "    optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    adv_op = optimizer2.minimize(-1 * adversial_fitness,\n",
    "                                global_step=tf.train.get_global_step(),\n",
    "                                var_list=gen_images)\n",
    "    numGradAscentIter = 5\n",
    "    def trainingContStep(iteration, session, training, cont_training):\n",
    "        (train_images, train_labels) = training\n",
    "        (cont_img, cont_labels) = cont_training\n",
    "\n",
    "        # generates the congingency\n",
    "        randomImages = nprandom.random((num_adversarial, num_input))\n",
    "        zerolabels = np.zeros(num_adversarial)\n",
    "        session.run(gen_images.assign(randomImages))\n",
    "\n",
    "        for iteration in range(numGradAscentIter):\n",
    "            #TODO does this work?\n",
    "            a = session.run(adv_op, feed_dict={\n",
    "                    gen_labels.name: zerolabels,\n",
    "                    #we are not training the weights!\n",
    "                    is_training.name: False})\n",
    "\n",
    "        adv_images = session.run(gen_images)\n",
    "        cont_img = np.concatenate((cont_img, adv_images), axis=0)\n",
    "        #TODO redo this, we can't switch contingency labels right now\n",
    "        cont_labels = np.zeros(len(cont_img))\n",
    "\n",
    "        a, t = session.run([acc, train_op], feed_dict={\n",
    "                features.name: train_images,\n",
    "                labels.name: train_labels,\n",
    "                cont_batch.name: cont_img,\n",
    "                cont_batch_la.name: cont_labels,\n",
    "                cont_beta.name: 1,\n",
    "                is_training.name: True})  \n",
    "\n",
    "        return (a, (adv_images, zerolabels))\n",
    "    return (acc, trainingContStep)\n",
    "\n",
    "def run(model_fn): \n",
    "    images = tf.placeholder(tf.float32, shape=[None, num_input], name=\"images\")\n",
    "    labels = tf.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    with tf.Session() as session:\n",
    "        (acc_eval, train_fn) = model_fn(images, labels, is_training)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        # Build the Estimator\n",
    "\n",
    "        for iteration in range(num_steps):\n",
    "            (training, cont_training) = next_batch(batch_size, (batch_size - num_adversarial))\n",
    "            (a, cont) = train_fn(iteration, session, training, cont_training)\n",
    "            (cont_imgs, cont_la) = cont\n",
    "            global contingency_imges\n",
    "            global contingency_labels\n",
    "            contingency_imges = np.concatenate((contingency_imges, cont_imgs), axis=0)\n",
    "            contingency_labels = np.concatenate((contingency_labels, cont_la), axis=0)\n",
    "            # a = session.run(accEval, feed_dict={images.name: train_im, labels.name: train_la})\n",
    "            if iteration % 50 == 0:\n",
    "                print(\"Generated contingency in iteration \", iteration, \":\", len(contingency_imges))\n",
    "                print(\"Training Accuracy in iteration \", iteration, \":\", a)\n",
    "\n",
    "        (eval_rel_im, eval_rel_la) = relabel(mnist.test)\n",
    "        (eval_valid_im, eval_valid_la) = only_valid(mnist.test.images, mnist.test.labels)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_rel_im, labels: eval_rel_la, is_training.name: False})\n",
    "        print(\"Final Accuracy on all relabeled classes\", iteration, \":\", a)\n",
    "        a = session.run(acc_eval, feed_dict={images: eval_valid_im, labels: eval_valid_la, is_training.name: False})\n",
    "        print(\"Final Accuracy on only valid classes\", iteration, \":\", a)\n",
    "        # TODO ROC-Courve\n",
    "\n",
    "def only_valid(images, labels):\n",
    "    indices = np.where(labels < num_classes )\n",
    "    return (images[indices], labels[indices])\n",
    "\n",
    "def relabel(dataset):\n",
    "    indices = np.where(dataset.labels >= num_classes )\n",
    "    relabel = np.copy(dataset.labels)\n",
    "    relabel[indices] = 0\n",
    "    return (dataset.images, relabel)\n",
    "\n",
    "def next_batch(batch_size_training, batch_size_contingency):\n",
    "    training = only_valid(*mnist.train.next_batch(batch_size))\n",
    "    indices = np.arange(0 , len(contingency_imges))\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:batch_size_contingency]\n",
    "    contingency_trainig_imgs = contingency_imges[indices]\n",
    "    contingency_trainig_labels = contingency_labels[indices]\n",
    "    contingency = (contingency_trainig_imgs, contingency_trainig_labels)\n",
    "    return (training, contingency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 10\n",
      "Training Accuracy in iteration  0 : 0.482759\n",
      "Generated contingency in iteration  50 : 510\n",
      "Training Accuracy in iteration  50 : 1.0\n",
      "Generated contingency in iteration  100 : 1010\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 1510\n",
      "Training Accuracy in iteration  150 : 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reset_contingency()\n",
    "run(withContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 0\n",
      "Training Accuracy in iteration  0 : 0.590909\n",
      "Generated contingency in iteration  50 : 0\n",
      "Training Accuracy in iteration  50 : 0.956522\n",
      "Generated contingency in iteration  100 : 0\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 0\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on all relabeled classes 199 : 0.7874\n",
      "Final Accuracy on only valid classes 199 : 0.999527\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reset_contingency()\n",
    "run(withoutContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated contingency in iteration  0 : 10\n",
      "Training Accuracy in iteration  0 : 0.633333\n",
      "Generated contingency in iteration  50 : 510\n",
      "Training Accuracy in iteration  50 : 0.961538\n",
      "Generated contingency in iteration  100 : 1010\n",
      "Training Accuracy in iteration  100 : 1.0\n",
      "Generated contingency in iteration  150 : 1510\n",
      "Training Accuracy in iteration  150 : 1.0\n",
      "Final Accuracy on all relabeled classes 199 : 0.8876\n",
      "Final Accuracy on only valid classes 199 : 0.995272\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reset_contingency()\n",
    "run(withRandomContingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_im, train_la) = only_zero_one(*mnist.train.next_batch(batch_size))\n",
    "train_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 784)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_imges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
